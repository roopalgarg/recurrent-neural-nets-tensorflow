{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download data from: \"http://www.cnts.ua.ac.be/conll2000/chunking/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/roopal/workspace/datasets/conll2000_chunking\"\n",
    "data_path_train = data_path + \"/train.txt\"\n",
    "data_path_test = data_path + \"/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_path_train,data_path_test, split_sentences=False):\n",
    "    df_lines = pd.read_csv(data_path_train, delim_whitespace=True, skip_blank_lines=False, header=None, names=[\"word\", \"tag\", \"ptag\"])\n",
    "    del df_lines[\"ptag\"]\n",
    "    \n",
    "    print df_lines.shape\n",
    "    \n",
    "    word2idx = dict()\n",
    "    wrd_idx = 0\n",
    "    \n",
    "    tag2idx = dict()\n",
    "    tag_idx = 0\n",
    "    \n",
    "    Xtrain = list()\n",
    "    Ytrain = list()\n",
    "    current_x = list()\n",
    "    current_y = list()\n",
    "    for _, row in df_lines.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        if word is not np.nan:\n",
    "            \n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = wrd_idx\n",
    "                wrd_idx += 1\n",
    "                \n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            \n",
    "            current_x.append(word2idx[word])\n",
    "            current_y.append(tag2idx[tag])\n",
    "        elif split_sentences:\n",
    "            Xtrain.append(current_x)\n",
    "            Ytrain.append(current_y)\n",
    "            current_x = list()\n",
    "            current_y = list()\n",
    "    \n",
    "    if not split_sentences:\n",
    "        Xtrain = current_x\n",
    "        Ytrain = current_y\n",
    "        \n",
    "    \n",
    "    df_lines = pd.read_csv(data_path_test, delim_whitespace=True, skip_blank_lines=False, header=None, names=[\"word\", \"tag\", \"ptag\"])\n",
    "    del df_lines[\"ptag\"]\n",
    "    \n",
    "    Xtest = list()\n",
    "    Ytest = list()\n",
    "    current_x = list()\n",
    "    current_y = list()\n",
    "    \n",
    "    for _, row in df_lines.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        \n",
    "        if word is not np.nan:\n",
    "            current_x.append(word2idx.get(word, wrd_idx)) # get index of unknown if word is unknown\n",
    "            current_y.append(tag2idx.get(tag, tag_idx))\n",
    "            \n",
    "        elif split_sentences:\n",
    "            Xtest.append(current_x)\n",
    "            Ytest.append(current_y)\n",
    "            current_x = list()\n",
    "            current_y = list()\n",
    "    \n",
    "    if not split_sentences:\n",
    "        Xtest = current_x\n",
    "        Ytest = current_y\n",
    "    \n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx, tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220663, 2)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, word2idx, tag2idx = get_data(data_path_train, data_path_test, split_sentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save word2idx for visualization\n",
    "with open(\"word2idx_pos_rnn.json\", 'w') as fp:\n",
    "    json.dump(word2idx, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936 8936 2012 2012\n"
     ]
    }
   ],
   "source": [
    "print len(Xtrain), len(Ytrain), len(Xtest), len(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8936,) (8936,)\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy arrays\n",
    "Xtrain = np.array(Xtrain)\n",
    "Ytrain = np.array(Ytrain)\n",
    "print (Xtrain.shape), (Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vocabulary size:', 19123)\n",
      "('Tags # ', 44)\n"
     ]
    }
   ],
   "source": [
    "N = len(Xtrain)\n",
    "V = len(word2idx) + 1 # + 1 represents the unknown words\n",
    "K = len(tag2idx)\n",
    "print (\"vocabulary size:\", V)\n",
    "print (\"Tags # \", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 15, 19, 20, 17, 21, 7, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
      "[0, 1, 2, 0, 3, 4, 5, 6, 7, 2, 8, 0, 1, 0, 9, 1, 10, 11, 8, 1, 0, 0, 11, 7, 6, 7, 2, 8, 0, 1, 10, 12, 10, 13, 8, 9, 14]\n"
     ]
    }
   ],
   "source": [
    "for words, tags in zip(Xtrain, Ytrain):\n",
    "    print words\n",
    "    print tags\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(Mi, Mo):\n",
    "        return np.random.rand(Mi, Mo)/ np.sqrt(Mi+Mo)\n",
    "    \n",
    "    def __init__(self, D, M, V, K, lr=10e-2):\n",
    "        \"\"\"\n",
    "        D: dimensionality of word embeddings\n",
    "        M: size of hidden layer\n",
    "        V: size of vocabulary\n",
    "        K: num of output classes\n",
    "        lr: learning rate\n",
    "        \"\"\"\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.V = V\n",
    "        self.K = K\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        with tf.name_scope(\"weights\"):\n",
    "            self.We = tf.Variable(tf.random_uniform([self.V, self.D], -1.0, 1.0), name=\"We\")\n",
    "            self.Wx = tf.Variable(RNN.init_weights(self.D, self.M) , dtype=tf.float32, name=\"Wx\")\n",
    "            self.Wh = tf.Variable(RNN.init_weights(self.M, self.M), dtype=tf.float32, name=\"Wh\")\n",
    "            self.Wo = tf.Variable(RNN.init_weights(self.M, self.K), dtype=tf.float32, name=\"Wo\")\n",
    "        \n",
    "            self.hist_We = tf.summary.histogram(\"hist_We\", self.We)\n",
    "            self.hist_Wx = tf.summary.histogram(\"hist_Wx\", self.Wh)\n",
    "            self.hist_Wh = tf.summary.histogram(\"hist_Wh\", self.Wx)\n",
    "            self.hist_Wo = tf.summary.histogram(\"hist_Wo\", self.Wo)\n",
    "        \n",
    "        with tf.name_scope(\"biases\"):\n",
    "            self.bh = tf.Variable(tf.zeros(shape=[1, self.M]), dtype=tf.float32, name=\"bh\")\n",
    "            self.bo = tf.Variable(tf.zeros(shape=[1, self.K]), dtype=tf.float32, name=\"bo\")\n",
    "            \n",
    "            self.hist_bh = tf.summary.histogram(\"hist_bh\", self.bh)\n",
    "            self.hist_bo = tf.summary.histogram(\"hist_bo\", self.bo)\n",
    "        \n",
    "        # initial hidden state\n",
    "        with tf.name_scope(\"initial_hidden_state\"):\n",
    "            self.h0 = tf.zeros(shape=[self.M], dtype=tf.float32, name=\"h0\")\n",
    "        \n",
    "        # input placeholder for sentences\n",
    "        self.input_seq = tf.placeholder(tf.int32, shape=(None), name=\"inputs\")\n",
    "        self.targets = tf.placeholder(tf.int32, shape=(None), name=\"targets\")\n",
    "        \n",
    "        self.tf_session = tf.Session()\n",
    "        \n",
    "        self.build_graph()\n",
    "        \n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(\"train_log/pos_rnn\" , self.tf_session.graph)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "    def build_graph(self):\n",
    "        _inputs = tf.nn.embedding_lookup(self.We, self.input_seq)\n",
    "        \n",
    "        self.ho = tf.scan(\n",
    "            fn=self._recurrence, elems=_inputs, initializer=self.h0, name=\"states\"\n",
    "        )\n",
    "        \n",
    "        self.py_x = tf.matmul(self.ho, self.Wo) + self.bo\n",
    "        \n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.py_x, self.targets), name=\"cost_func\")\n",
    "            tf.summary.scalar(\"cross_entropy\", self.cost)\n",
    "            \n",
    "        with tf.name_scope(\"train\"):   \n",
    "            trainables = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.cost, trainables)\n",
    "            \n",
    "            grads, _ = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "            grad_var_pairs = zip(grads, trainables)\n",
    "            \n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "            self.train_op = opt.apply_gradients(grad_var_pairs)\n",
    "        \n",
    "    def _recurrence(self, h_t_minus_1, x_t):\n",
    "        x_t = tf.reshape(x_t, [1, self.D])\n",
    "        h_t_minus_1 = tf.reshape(h_t_minus_1,[1, self.M])\n",
    "        \n",
    "        h_t = tf.nn.tanh(\n",
    "            tf.matmul(x_t, self.Wx) + tf.matmul(h_t_minus_1, self.Wh) + self.bh\n",
    "        )\n",
    "        \n",
    "        h_t = tf.reshape(h_t, [self.M], name=\"h\")\n",
    "        return h_t\n",
    "    \n",
    "    def predict(self, input_seq):\n",
    "        self.input_seq = input_seq\n",
    "        \n",
    "        py_x = self.tf_session.run(self.py_x)\n",
    "        \n",
    "        return np.argmax(py_x, axis=1)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=500):\n",
    "        # num of sentences\n",
    "        N = len(X)\n",
    "        \n",
    "        print (\"Initializing global variables\")\n",
    "        self.tf_session.run(tf.global_variables_initializer())\n",
    "        print (\"# of trainable var outside: \" + str(len(tf.trainable_variables())))\n",
    "        \n",
    "        net_idx = 0\n",
    "        costs = list()\n",
    "        for idx_epoch in xrange(epochs):\n",
    "            cost_epoch = 0\n",
    "            for idx_sent in xrange(N):\n",
    "                print (\"epoch: {}, sentence: {}\".format(idx_epoch, idx_sent))\n",
    "                \n",
    "                targets = Y[idx_sent]\n",
    "                feed_dict = {self.input_seq: X[idx_sent], self.targets: targets}\n",
    "                \n",
    "                self.tf_session.run(self.train_op, feed_dict=feed_dict)\n",
    "                \n",
    "                py_x, cost, We = self.tf_session.run([self.py_x, self.cost, self.We], feed_dict=feed_dict)\n",
    "                \n",
    "                pred = np.argmax(py_x, axis=1)\n",
    "                print \"Y: \", targets\n",
    "                print \"Prediction: \", pred\n",
    "                \n",
    "                accuracy = 0\n",
    "                for y, y_ in zip(targets, pred):\n",
    "                    if y==y_:\n",
    "                        accuracy += 1 \n",
    "                accuracy = float(accuracy)/len(pred)\n",
    "                \n",
    "                summary = self.tf_session.run(self.summary_op, feed_dict=feed_dict)\n",
    "                self.train_writer.add_summary(summary, net_idx)\n",
    "                self.train_writer.flush()\n",
    "                \n",
    "                print \"Accuracy Sentence # {}\".format(idx_sent), accuracy\n",
    "                print \"Cost Sentence # {}\".format(idx_sent), cost\n",
    "                cost_epoch += cost\n",
    "                \n",
    "                net_idx += 1\n",
    "            costs.append(cost_epoch)\n",
    "            \n",
    "            print (\"---------\")\n",
    "            print (\"Cost at epoch {} is {}\".format(idx_epoch, cost_epoch))\n",
    "            print (\"---------\")\n",
    "            \n",
    "    def save_model(self, model_name):\n",
    "        \n",
    "        self.saver.save(self.tf_session, model_name)\n",
    "        self.save_embedding_matrix()\n",
    "        \n",
    "    def save_embedding_matrix(self):\n",
    "        file_path = \"word_embedding_pos_rnn.npy\"\n",
    "        np.save(file_path, self.We.eval(self.tf_session))\n",
    "    \n",
    "    def load_model(self, model_name):\n",
    "        \"\"\"\n",
    "        doesnt work yet!\n",
    "        :param model_name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.tf_session, model_name)\n",
    "        print (self.We.eval(self.tf_session))\n",
    "\n",
    "    def close_session(self):\n",
    "        self.train_writer.close()\n",
    "        self.tf_session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = RNN(300, 10, V, K, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_epochs = 5\n",
    "    rnn.fit(Xtrain, Ytrain, epochs=num_epochs)\n",
    "    rnn.save_model(\"model_rnn_pos\")\n",
    "finally:\n",
    "    print (\"Closing Session\")\n",
    "    rnn.close_session()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
