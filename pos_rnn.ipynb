{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to get the data\n",
    "#### download data from: \"http://www.cnts.ua.ac.be/conll2000/chunking/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/roopal/workspace/datasets/conll2000_chunking\"\n",
    "data_path_train = data_path + \"/train.txt\"\n",
    "data_path_test = data_path + \"/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_path_train,data_path_test, split_sentences=False):\n",
    "    df_lines = pd.read_csv(data_path_train, delim_whitespace=True, skip_blank_lines=False, header=None, names=[\"word\", \"tag\", \"ptag\"])\n",
    "    del df_lines[\"ptag\"]\n",
    "    \n",
    "    word2idx = dict()\n",
    "    wrd_idx = 0\n",
    "    \n",
    "    tag2idx = dict()\n",
    "    tag_idx = 0\n",
    "    \n",
    "    Xtrain = list()\n",
    "    Ytrain = list()\n",
    "    current_x = list()\n",
    "    current_y = list()\n",
    "    for _, row in df_lines.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        if word is not np.nan:\n",
    "            \n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = wrd_idx\n",
    "                wrd_idx += 1\n",
    "                \n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            \n",
    "            current_x.append(word2idx[word])\n",
    "            current_y.append(tag2idx[tag])\n",
    "        elif split_sentences:\n",
    "            Xtrain.append(current_x)\n",
    "            Ytrain.append(current_y)\n",
    "            current_x = list()\n",
    "            current_y = list()\n",
    "    \n",
    "    if not split_sentences:\n",
    "        Xtrain = current_x\n",
    "        Ytrain = current_y\n",
    "        \n",
    "    \n",
    "    df_lines = pd.read_csv(data_path_test, delim_whitespace=True, skip_blank_lines=False, header=None, names=[\"word\", \"tag\", \"ptag\"])\n",
    "    del df_lines[\"ptag\"]\n",
    "    \n",
    "    Xtest = list()\n",
    "    Ytest = list()\n",
    "    current_x = list()\n",
    "    current_y = list()\n",
    "    \n",
    "    for _, row in df_lines.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        \n",
    "        if word is not np.nan:\n",
    "            current_x.append(word2idx.get(word, wrd_idx)) # get index of unknown if word is unknown\n",
    "            current_y.append(tag2idx.get(tag, tag_idx))\n",
    "            \n",
    "        elif split_sentences:\n",
    "            Xtest.append(current_x)\n",
    "            Ytest.append(current_y)\n",
    "            current_x = list()\n",
    "            current_y = list()\n",
    "    \n",
    "    if not split_sentences:\n",
    "        Xtest = current_x\n",
    "        Ytest = current_y\n",
    "    \n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx, tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, word2idx, tag2idx = get_data(data_path_train, data_path_test, split_sentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936 8936 2012 2012 19122 44\n"
     ]
    }
   ],
   "source": [
    "print len(Xtrain), len(Ytrain), len(Xtest), len(Ytest), len(word2idx), len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 15, 19, 20, 17, 21, 7, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33] [0, 1, 2, 0, 3, 4, 5, 6, 7, 2, 8, 0, 1, 0, 9, 1, 10, 11, 8, 1, 0, 0, 11, 7, 6, 7, 2, 8, 0, 1, 10, 12, 10, 13, 8, 9, 14]\n"
     ]
    }
   ],
   "source": [
    "print Xtrain[0], Ytrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save word2idx for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2idx_pos_rnn.json\", 'w') as fp:\n",
    "    json.dump(word2idx, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8936,) (8936,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = np.array(Xtrain)\n",
    "Ytrain = np.array(Ytrain)\n",
    "print (Xtrain.shape), (Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vocabulary size:', 19123)\n",
      "('Tags # ', 44)\n"
     ]
    }
   ],
   "source": [
    "N = len(Xtrain)\n",
    "V = len(word2idx) + 1 # + 1 represents the unknown words\n",
    "K = len(tag2idx)\n",
    "print (\"vocabulary size:\", V)\n",
    "print (\"Tags # \", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(Mi, Mo):\n",
    "        return np.random.rand(Mi, Mo)/ np.sqrt(Mi+Mo)\n",
    "    \n",
    "    def __init__(self, D, M, V, K, lr=10e-2):\n",
    "        \"\"\"\n",
    "        D: dimensionality of word embeddings\n",
    "        M: size of hidden layer\n",
    "        V: size of vocabulary\n",
    "        K: num of output classes\n",
    "        lr: learning rate\n",
    "        \"\"\"\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.V = V\n",
    "        self.K = K\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        \"\"\"\n",
    "        RNN weights and biases\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            self.We = tf.Variable(tf.random_uniform([self.V, self.D], -1.0, 1.0), name=\"We\")\n",
    "            self.Wx = tf.Variable(RNN.init_weights(self.D, self.M) , dtype=tf.float32, name=\"Wx\")\n",
    "            self.Wh = tf.Variable(RNN.init_weights(self.M, self.M), dtype=tf.float32, name=\"Wh\")\n",
    "            self.Wo = tf.Variable(RNN.init_weights(self.M, self.K), dtype=tf.float32, name=\"Wo\")\n",
    "        \n",
    "            self.hist_We = tf.summary.histogram(\"hist_We\", self.We)\n",
    "            self.hist_Wx = tf.summary.histogram(\"hist_Wx\", self.Wh)\n",
    "            self.hist_Wh = tf.summary.histogram(\"hist_Wh\", self.Wx)\n",
    "            self.hist_Wo = tf.summary.histogram(\"hist_Wo\", self.Wo)\n",
    "        \n",
    "        with tf.name_scope(\"biases\"):\n",
    "            self.bh = tf.Variable(tf.zeros(shape=[1, self.M]), dtype=tf.float32, name=\"bh\")\n",
    "            self.bo = tf.Variable(tf.zeros(shape=[1, self.K]), dtype=tf.float32, name=\"bo\")\n",
    "            \n",
    "            self.hist_bh = tf.summary.histogram(\"hist_bh\", self.bh)\n",
    "            self.hist_bo = tf.summary.histogram(\"hist_bo\", self.bo)\n",
    "        \n",
    "        # initial hidden state\n",
    "        with tf.name_scope(\"initial_hidden_state\"):\n",
    "            self.h0 = tf.zeros(shape=[self.M], dtype=tf.float32, name=\"h0\")\n",
    "        \n",
    "        # input placeholder for sentences\n",
    "        self.input_seq = tf.placeholder(tf.int32, shape=(None), name=\"inputs\")\n",
    "        self.targets = tf.placeholder(tf.int32, shape=(None), name=\"targets\")\n",
    "        \n",
    "        self.tf_session = tf.Session()\n",
    "        \n",
    "        self.build_graph()\n",
    "        \n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(\"train_log/pos_rnn\" , self.tf_session.graph)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        define the tensorflow ops\n",
    "        \"\"\"\n",
    "        _inputs = tf.nn.embedding_lookup(self.We, self.input_seq)\n",
    "        \n",
    "        \"\"\"\n",
    "        the scan function helps run a loop over the input\n",
    "        \"\"\"\n",
    "        self.ho = tf.scan(\n",
    "            fn=self._recurrence, elems=_inputs, initializer=self.h0, name=\"states\"\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        the predictions\n",
    "        \"\"\"\n",
    "        self.py_x = tf.matmul(self.ho, self.Wo) + self.bo\n",
    "        \n",
    "        \"\"\"\n",
    "        calculate the loss/cost\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.py_x, labels=self.targets), name=\"cost_func\")\n",
    "            tf.summary.scalar(\"cross_entropy\", self.cost)\n",
    "            \n",
    "        \"\"\"\n",
    "        calculate the gradients and do the weight updates\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"train\"):   \n",
    "            trainables = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.cost, trainables)\n",
    "            \n",
    "            grads, _ = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "            grad_var_pairs = zip(grads, trainables)\n",
    "            \n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "            self.train_op = opt.apply_gradients(grad_var_pairs)\n",
    "        \n",
    "    def _recurrence(self, h_t_minus_1, x_t):\n",
    "        x_t = tf.reshape(x_t, [1, self.D])\n",
    "        h_t_minus_1 = tf.reshape(h_t_minus_1,[1, self.M])\n",
    "        \n",
    "        h_t = tf.nn.tanh(\n",
    "            tf.matmul(x_t, self.Wx) + tf.matmul(h_t_minus_1, self.Wh) + self.bh\n",
    "        )\n",
    "        \n",
    "        h_t = tf.reshape(h_t, [self.M], name=\"h\")\n",
    "        return h_t\n",
    "    \n",
    "    def predict(self, input_seq):\n",
    "        self.input_seq = input_seq\n",
    "        \n",
    "        py_x = self.tf_session.run(self.py_x)\n",
    "        \n",
    "        return np.argmax(py_x, axis=1)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=500):\n",
    "        # num of sentences\n",
    "        N = len(X)\n",
    "        \n",
    "        print (\"Initializing global variables\")\n",
    "        self.tf_session.run(tf.global_variables_initializer())\n",
    "        print (\"# of trainable var outside: \" + str(len(tf.trainable_variables())))\n",
    "        \n",
    "        net_idx = 0\n",
    "        costs = list()\n",
    "        for idx_epoch in xrange(epochs):\n",
    "            cost_epoch = 0\n",
    "            for idx_sent in xrange(N):\n",
    "    \n",
    "                targets = Y[idx_sent]\n",
    "                feed_dict = {self.input_seq: X[idx_sent], self.targets: targets}\n",
    "                \n",
    "                self.tf_session.run(self.train_op, feed_dict=feed_dict)\n",
    "                \n",
    "                py_x, cost, We = self.tf_session.run([self.py_x, self.cost, self.We], feed_dict=feed_dict)\n",
    "                \n",
    "                pred = np.argmax(py_x, axis=1)\n",
    "                \n",
    "                accuracy = 0\n",
    "                for y, y_ in zip(targets, pred):\n",
    "                    if y==y_:\n",
    "                        accuracy += 1 \n",
    "                accuracy = float(accuracy)/len(pred)\n",
    "                \n",
    "                summary = self.tf_session.run(self.summary_op, feed_dict=feed_dict)\n",
    "                self.train_writer.add_summary(summary, net_idx)\n",
    "                self.train_writer.flush()\n",
    "                \n",
    "                if idx_sent % 500 == 0:\n",
    "                    print (\"epoch: {}, sentence: {}\".format(idx_epoch, idx_sent))\n",
    "                    print \"Y: \", targets\n",
    "                    print \"Prediction: \", pred\n",
    "                    print \"Accuracy Sentence # {}\".format(idx_sent), accuracy\n",
    "                    print \"Cost Sentence # {}\".format(idx_sent), cost\n",
    "                \n",
    "                cost_epoch += cost\n",
    "                \n",
    "                net_idx += 1\n",
    "            costs.append(cost_epoch)\n",
    "            \n",
    "            print (\"---------\")\n",
    "            print (\"Cost at epoch {} is {}\".format(idx_epoch, cost_epoch))\n",
    "            print (\"---------\")\n",
    "            \n",
    "    def save_model(self, model_name):\n",
    "        \n",
    "        self.saver.save(self.tf_session, model_name)\n",
    "        self.save_embedding_matrix()\n",
    "        \n",
    "    def save_embedding_matrix(self):\n",
    "        file_path = \"word_embedding_pos_rnn.npy\"\n",
    "        np.save(file_path, self.We.eval(self.tf_session))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_latest_model_path(self, save_dir):\n",
    "        checkpoint_state = tf.train.get_checkpoint_state(save_dir)\n",
    "        latest_model_path = checkpoint_state.model_checkpoint_path\n",
    "        return latest_model_path\n",
    "\n",
    "    def restore_latest_model(self, save_dir):\n",
    "        latest_model_path = RNN.get_latest_model_path(save_dir)\n",
    "        logging.info(\"loading model from {}\".format(latest_model_path))\n",
    "        self.saver.restore(self.tf_session, latest_model_path)\n",
    "\n",
    "    def close_session(self):\n",
    "        self.train_writer.close()\n",
    "        self.tf_session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(300, 10, V, K, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing global variables\n",
      "# of trainable var outside: 6\n",
      "epoch: 0, sentence: 0\n",
      "Y:  [0, 1, 2, 0, 3, 4, 5, 6, 7, 2, 8, 0, 1, 0, 9, 1, 10, 11, 8, 1, 0, 0, 11, 7, 6, 7, 2, 8, 0, 1, 10, 12, 10, 13, 8, 9, 14]\n",
      "Prediction:  [14 14 14  2  8 39  7  7  7  7 39  8  8  7 39 30 39  7  7  9 14 14 14 14 14\n",
      "  9 25 14  1 14 14  9  2 21 34  1  2]\n",
      "Accuracy Sentence # 0 0.027027027027\n",
      "Cost Sentence # 0 3.77149\n",
      "epoch: 0, sentence: 500\n",
      "Y:  [2, 8, 0, 3, 16, 2, 5, 0, 0, 1, 10, 28, 0, 10, 10, 34, 23, 7, 0, 0, 6, 9, 1, 16, 2, 0, 1, 8, 9, 11, 16, 6, 9, 8, 1, 2, 0, 14]\n",
      "Prediction:  [11  0  0 14  0  2  0  0  0  1  0  0  0  0  0  1  0  0  0  0 14  0  0  0  0\n",
      "  0  1  0  0 11  0 14  0  0  0  2  0 14]\n",
      "Accuracy Sentence # 500 0.368421052632\n",
      "Cost Sentence # 500 2.53273\n",
      "epoch: 0, sentence: 1000\n",
      "Y:  [2, 0, 21, 29, 3, 4, 7, 2, 8, 0, 13, 0, 6, 4, 7, 2, 0, 13, 16, 8, 0, 9, 14]\n",
      "Prediction:  [ 2  0  0  0  0  0  1  2  0  0  0  0  6  0  0  2  0  0  0  0  0  0 14]\n",
      "Accuracy Sentence # 1000 0.434782608696\n",
      "Cost Sentence # 1000 2.12036\n",
      "epoch: 0, sentence: 1500\n",
      "Y:  [8, 1, 29, 15, 9, 14]\n",
      "Prediction:  [ 9  1  0 15  1 14]\n",
      "Accuracy Sentence # 1500 0.5\n",
      "Cost Sentence # 1500 1.83306\n",
      "epoch: 0, sentence: 2000\n",
      "Y:  [8, 9, 23, 7, 33, 1, 2, 8, 0, 14]\n",
      "Prediction:  [ 1  0 10  0  0  1  2  0  0 14]\n",
      "Accuracy Sentence # 2000 0.4\n",
      "Cost Sentence # 2000 2.18801\n",
      "epoch: 0, sentence: 2500\n",
      "Y:  [29, 21, 29, 7, 2, 0, 1, 16, 8, 9, 1, 2, 0, 39, 12, 1, 16, 1, 2, 0, 12, 0, 11, 29, 3, 5, 17, 0, 1, 31, 3, 4, 2, 8, 0, 1, 2, 9, 14]\n",
      "Prediction:  [29 10  0  0  2  0  1  0  0  0  1  2  0  0 12  1  0  1  2  0 12 10 11 29  3\n",
      "  0 13  0  1  0  3 10  2  0  0  1  2  0 14]\n",
      "Accuracy Sentence # 2500 0.615384615385\n",
      "Cost Sentence # 2500 1.59611\n",
      "epoch: 0, sentence: 3000\n",
      "Y:  [8, 8, 9, 15, 2, 9, 1, 2, 30, 11, 8, 12, 36, 8, 9, 1, 16, 9, 8, 6, 29, 14]\n",
      "Prediction:  [ 4 10  9  0  2  0  1  2  0 11  0 12  2  0  0  1  9  0  0  6 18 14]\n",
      "Accuracy Sentence # 3000 0.409090909091\n",
      "Cost Sentence # 3000 1.99316\n",
      "epoch: 0, sentence: 3500\n",
      "Y:  [10, 10, 10, 11, 0, 1, 10, 10, 11, 2, 10, 11, 10, 11, 0, 34, 3, 8, 9, 11, 21, 2, 8, 0, 1, 10, 10, 13, 0, 1, 33, 0, 11, 5, 4, 1, 40, 14]\n",
      "Prediction:  [ 0  0  0 11  8  1  9 10 11  2  0 11  0 11 10  1  0  0  0 11  0  2  0  0  1\n",
      "  9  8 13  8  1 10  0 11  0  8  1 10 14]\n",
      "Accuracy Sentence # 3500 0.447368421053\n",
      "Cost Sentence # 3500 1.6416\n",
      "epoch: 0, sentence: 4000\n",
      "Y:  [19, 29, 15, 29, 3, 6, 10, 13, 0, 6, 7, 5, 1, 2, 8, 0, 11, 20, 21, 10, 10, 11, 0, 0, 1, 10, 11, 2, 10, 0, 34, 3, 2, 10, 0, 14]\n",
      "Prediction:  [19 29  3 29 13  6 18 13  0  6  7 10  1  2  0 10 11 20 21 10  0 11  0  0  1\n",
      " 10 11  2  0  0  1  3  2  0 10 14]\n",
      "Accuracy Sentence # 4000 0.694444444444\n",
      "Cost Sentence # 4000 0.940928\n",
      "epoch: 0, sentence: 4500\n",
      "Y:  [10, 10, 10, 1, 10, 0, 21, 18, 9, 2, 0, 6, 26, 18, 14]\n",
      "Prediction:  [10  0 10  1  0  0 21  0  9  2  0  6 26 18 14]\n",
      "Accuracy Sentence # 4500 0.8\n",
      "Cost Sentence # 4500 0.944856\n",
      "epoch: 0, sentence: 5000\n",
      "Y:  [4, 11, 16, 10, 0, 9, 15, 4, 16, 29, 1, 2, 8, 9, 31, 4, 15, 1, 2, 0, 14]\n",
      "Prediction:  [ 4 11  4 10  0 10 15 10 10  0  1  2  0  0 23  4 10  1  2  0 14]\n",
      "Accuracy Sentence # 5000 0.571428571429\n",
      "Cost Sentence # 5000 1.61723\n",
      "epoch: 0, sentence: 5500\n",
      "Y:  [8, 9, 1, 2, 8, 0, 39, 18, 0, 18, 0, 39, 18, 0, 18, 9, 39, 18, 0, 18, 9, 14]\n",
      "Prediction:  [21  9  1  2  0  0 39  0  0 18  0  1 18  0 18  9  0  9  0  4  9 14]\n",
      "Accuracy Sentence # 5500 0.681818181818\n",
      "Cost Sentence # 5500 1.1227\n",
      "epoch: 0, sentence: 6000\n",
      "Y:  [12, 0, 21, 29, 4, 12, 36, 4, 15, 2, 9, 15, 8, 14]\n",
      "Prediction:  [12  4 10 29  4 12 18  9 10  2  4 15 18 14]\n",
      "Accuracy Sentence # 6000 0.5\n",
      "Cost Sentence # 6000 1.73069\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_epochs = 5\n",
    "    rnn.fit(Xtrain, Ytrain, epochs=num_epochs)\n",
    "    rnn.save_model(\"model_rnn_pos\")\n",
    "finally:\n",
    "    print (\"Closing Session\")\n",
    "    rnn.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
